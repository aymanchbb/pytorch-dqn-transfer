import gymnasium as gym
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import deque
import random


# ------- Let's initialise our project ----------

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
env = gym.make('CartPole-v1')
LR = 0.0005
QTRAIN_SIZE = 128
GAMMA = 0.99
ETA = 0.002
EPS_END = 0.05
EPS_START = 0.9
EPS_DECROISSANCE = 5000
# ------ Brain definition ------------------
class DQN(nn.Module):
  def __init__(self, n_observations, n_actions):
    super(DQN, self).__init__()
    self.layer1 = nn.Linear(n_observations, 128)
    self.layer2 = nn.Linear(128, 128)
    self.layer3 = nn.Linear(128,128)
    self.layer4 = nn.Linear(128, n_actions)

  def forward(self, x):
    x = F.relu(self.layer1(x))
    x = F.relu(self.layer2(x))
    x = F.relu(self.layer3(x))
    return self.layer4(x)

# -------- init ----------
n_actions = env.action_space.n
n_observations = env.observation_space.shape[0]
policy_net = DQN(n_observations, n_actions).to(device)
target_net = DQN(n_observations, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict())
optimizer = torch.optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
memory = deque([], maxlen=10000)

# ------ Training function -------
def optimize_model():
  if QTRAIN_SIZE > len(memory):
    return

  transitions = random.sample(memory, QTRAIN_SIZE)
  batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)

  state_batch = torch.cat(batch_state)
  action_batch = torch.tensor(batch_action, device = device).unsqueeze(1)
  reward_batch = torch.tensor(batch_reward, device=device).unsqueeze(1)
  non_final_next_states = torch.cat([s for s in batch_next_state if s is not None])
  done_batch = torch.tensor(batch_done, device=device, dtype = torch.int).unsqueeze(1)

  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=device, dtype=torch.bool)

  q_actuel = policy_net(state_batch).gather(1,action_batch)
  next_state_values = torch.zeros(QTRAIN_SIZE, device=device)

  with torch.no_grad():
    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]

  q_cible = reward_batch + ((1 - done_batch)*next_state_values.unsqueeze(1) * GAMMA)

  crit = nn.SmoothL1Loss()
  loss = crit(q_actuel, q_cible)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()


# ----- Effective training ----------

steps_done = 0
for i_episode in range(600):
  state, info = env.reset()
  state = torch.tensor(state, device=device).unsqueeze(0)
  total_reward = 0 

  for t in range(1000):
    epsilon = EPS_END + (EPS_START - EPS_END)*math.exp((-steps_done)/EPS_DECROISSANCE) 
    steps_done += 1
    smpl = random.random() 

    if smpl > epsilon:
      with torch.no_grad() : 
        action = policy_net(state).max(1)[1].view(1,1)
    else: 
      action = torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)
    
    ob, reward, terminated , finished, info = env.step(action.item())

    total_reward += reward

    next_step = torch.tensor(ob, device = device, dtype=torch.float32).unsqueeze(0) if not terminated else None

    memory.append((state, action.item(), reward, next_step,  (terminated or finished))) 

    state = next_step
    optimize_model() 

    target_net_state_dict = target_net.state_dict()
    for par in target_net.state_dict():
      target_net_state_dict[par] = (ETA*policy_net.state_dict()[par] +(1-ETA)*target_net.state_dict()[par])  
    target_net.load_state_dict(target_net_state_dict)

    if (terminated or finished):
      if i_episode%10 == 0: print(f"Ep {i_episode}: Score {total_reward}")
      break






