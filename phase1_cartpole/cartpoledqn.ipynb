import gymnasium as gym
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import deque
import random



# ------- Let's initialise our project ----------
seed_val = 42
random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed(seed_val)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
env = gym.make('CartPole-v1')
LR = 0.001
QTRAIN_SIZE = 128
GAMMA = 0.99
ETA = 0.002
EPS_END = 0.05
EPS_START = 0.9
EPS_DECROISSANCE = 2000
# ------ Brain definition ------------------
class DQN(nn.Module):
  def __init__(self, n_observations, n_actions):
    super(DQN, self).__init__()
    self.layer1 = nn.Linear(n_observations, 128)
    self.layer2 = nn.Linear(128, 128)
    self.layer3 = nn.Linear(128,128)
    self.layer4 = nn.Linear(128, n_actions)

  def forward(self, x):
    x = F.relu(self.layer1(x))
    x = F.relu(self.layer2(x))
    x = F.relu(self.layer3(x))
    return self.layer4(x)

# -------- init ----------
n_actions = env.action_space.n
n_observations = env.observation_space.shape[0]
policy_net = DQN(n_observations, n_actions).to(device)
target_net = DQN(n_observations, n_actions).to(device)
target_net.load_state_dict(policy_net.state_dict())
optimizer = torch.optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
memory = deque([], maxlen=30000)

# ------ Training function -------
def optimize_model():
  if QTRAIN_SIZE > len(memory):
    return

  transitions = random.sample(memory, QTRAIN_SIZE)
  batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)

  state_batch = torch.cat(batch_state)
  action_batch = torch.tensor(batch_action, device = device).unsqueeze(1)
  reward_batch = torch.tensor(batch_reward, device=device).unsqueeze(1)
  non_final_next_states = torch.cat([s for s in batch_next_state if s is not None])
  done_batch = torch.tensor(batch_done, device=device, dtype = torch.int).unsqueeze(1)

  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=device, dtype=torch.bool)

  q_actuel = policy_net(state_batch).gather(1,action_batch)
  next_state_values = torch.zeros(QTRAIN_SIZE, device=device)

  with torch.no_grad():
    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]

  q_cible = reward_batch + ((1 - done_batch)*next_state_values.unsqueeze(1) * GAMMA)

  crit = nn.SmoothL1Loss()
  loss = crit(q_actuel, q_cible)
  optimizer.zero_grad()
  loss.backward()
  torch.nn.utils.clip_grad_value_(policy_net.parameters(), 1.0)
  optimizer.step()


# ----- Effective training ----------

steps_done = 0
reward_list = []
for i_episode in range(600):
  state, info = env.reset(seed=seed_val)
  state = torch.tensor(state, device=device).unsqueeze(0)
  total_reward = 0 

  for t in range(1000):
    epsilon = EPS_END + (EPS_START - EPS_END)*math.exp((-steps_done)/EPS_DECROISSANCE) 
    steps_done += 1
    smpl = random.random() 

    if smpl > epsilon:
      with torch.no_grad() : 
        action = policy_net(state).max(1)[1].view(1,1)
    else: 
      action = torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)
    
    ob, reward, terminated , finished, info = env.step(action.item())

    total_reward += reward

    next_step = torch.tensor(ob, device = device, dtype=torch.float32).unsqueeze(0) if not terminated else None

    memory.append((state, action.item(), reward, next_step,  (terminated or finished))) 

    state = next_step
    optimize_model() 

    target_net_state_dict = target_net.state_dict()
    for par in target_net.state_dict():
      target_net_state_dict[par] = (ETA*policy_net.state_dict()[par] +(1-ETA)*target_net.state_dict()[par])  
    target_net.load_state_dict(target_net_state_dict)

    if (terminated or finished):
      if i_episode%10 == 0: print(f"Ep {i_episode}: Score {total_reward}")
      break
  
  reward_list.append(total_reward)

# Plot the graph

import matplotlib.pyplot as plt

scores_tensor = torch.tensor(reward_list, dtype=torch.float)

plt.style.use('seaborn-v0_8-darkgrid') 
plt.figure(figsize=(12, 6))
              
plt.plot(scores_tensor.numpy(), label='Score', color='cyan', alpha=0.3)
              

window_size = 50
if len(reward_list) >= window_size:

  unfolded = scores_tensor.unfold(0, window_size, 1)
  means = unfolded.mean(1).numpy()
                  
  plt.plot(range(window_size - 1, len(reward_list)), means, label=f'Average ({window_size} games)', color='blue', linewidth=3)

plt.title('Learning progress', fontsize=16)
plt.ylabel('Score', fontsize=14)
plt.xlabel('Games', fontsize=14)
plt.legend(loc='lower right', fontsize=12)
plt.show()
plt.savefig("progress_graph_dqn_cartpole_dqn.png") 





