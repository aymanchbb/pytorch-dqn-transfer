import gymnasium as gym
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import deque
import random
import os

# We are going to adapt our previous code of course 

# ------- Let's initialise our project ----------
seed_val = 42
random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed(seed_val)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
env = gym.make('LunarLander-v3')
LR = 0.0005
LR_END = 0.0001
QTRAIN_SIZE = 128
GAMMA = 0.99
ETA = 0.002
EPS_END = 0.05
EPS_START = 0.9
EPS_DECROISSANCE = 50000
TARGET_SCORE = 150
WINDOW = 10
# ------ Brain definition ------------------
class New_DQN(nn.Module):
  def __init__(self, n_observations, n_actions):
    super(New_DQN, self).__init__()
    self.first_layer = nn.Linear(n_observations, 128)

    self.recycled2 = nn.Linear(128, 128)
    self.recycled3 = nn.Linear(128,128)

    self.last_layer = nn.Linear(128, n_actions)

  def forward(self, x):
    x = F.relu(self.first_layer(x))
    x = F.relu(self.recycled2(x))
    x = F.relu(self.recycled3(x))
    return self.last_layer(x)

# -------- init ----------
n_actions = env.action_space.n
n_observations = env.observation_space.shape[0]
policy_net = New_DQN(n_observations, n_actions).to(device)
  # Freezeing the old brain 
target_net = New_DQN(n_observations, n_actions).to(device)
optimizer = torch.optim.AdamW([
    {
        'params': list(policy_net.first_layer.parameters()) + list(policy_net.last_layer.parameters()),
        'lr': 0.001  
    },
    {
        'params': list(policy_net.recycled2.parameters()) + list(policy_net.recycled3.parameters()),
        'lr': 0.00001 
    }
], amsgrad=True)
memory = deque([], maxlen=100000)
target_net.load_state_dict(policy_net.state_dict())

# ------ Brain Surgery -------------

new_weights = policy_net.state_dict()
weights_file = "cartpole_dqn_weights.pth"

if os.path.exists(weights_file):
  print("Loading weights")
  old_state_dict = torch.load(weights_file)

  mapp = {
      "recycled2.weight": "layer2.weight",
      "recycled2.bias": "layer2.bias",
      "recycled3.weight": "layer3.weight",
      "recycled3.bias": "layer3.bias"
  }

  with torch.no_grad():
    for new_key, old_key in mapp.items():
      if new_key in new_weights:
        new_weights[new_key] = old_state_dict[old_key]
        print(f"we grafted {old_key} in {new_key}")
      else:
        raise Exception(f"{new_key} n'existe pas")
else:
  raise Exception("No weight file found")

policy_net.load_state_dict(new_weights)
target_net.load_state_dict(policy_net.state_dict())


# ------ Training function -------
def optimize_model():
  if QTRAIN_SIZE > len(memory):
    return

  transitions = random.sample(memory, QTRAIN_SIZE)
  batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)

  state_batch = torch.cat(batch_state)
  action_batch = torch.tensor(batch_action, device = device).unsqueeze(1)
  reward_batch = torch.tensor(batch_reward, device=device).unsqueeze(1)
  non_final_next_states = torch.cat([s for s in batch_next_state if s is not None])
  done_batch = torch.tensor(batch_done, device=device, dtype = torch.int).unsqueeze(1)

  non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=device, dtype=torch.bool)

  q_actuel = policy_net(state_batch).gather(1,action_batch)
  next_state_values = torch.zeros(QTRAIN_SIZE, device=device)

  with torch.no_grad():
    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]

  q_cible = reward_batch + ((1 - done_batch)*next_state_values.unsqueeze(1) * GAMMA)

  crit = nn.SmoothL1Loss()
  loss = crit(q_actuel, q_cible)
  optimizer.zero_grad()
  loss.backward()
  torch.nn.utils.clip_grad_value_(policy_net.parameters(), 1.0)
  optimizer.step()


# ----- Effective training ----------

steps_done = 0
reward_list = []
for i_episode in range(600):
  current_seed = seed_val + i_episode
  state, info = env.reset(seed=current_seed)
  state = torch.tensor(state, device=device).unsqueeze(0)
  total_reward = 0
  

  for t in range(1000):
    epsilon = EPS_END + (EPS_START - EPS_END)*math.exp((-steps_done)/EPS_DECROISSANCE)
    steps_done += 1
    smpl = random.random()

    if smpl > epsilon:
      with torch.no_grad() :
        action = policy_net(state).max(1)[1].view(1,1)
    else:
      random_action = random.randint(0, n_actions-1)
      action = torch.tensor([[random_action]], device=device, dtype=torch.long)

    ob, reward, terminated , finished, info = env.step(action.item())

    total_reward += reward

    next_step = torch.tensor(ob, device = device, dtype=torch.float32).unsqueeze(0) if not terminated else None

    memory.append((state, action.item(), reward, next_step,  (terminated )))

    state = next_step
    optimize_model()

    target_net_state_dict = target_net.state_dict()
    for par in target_net.state_dict():
      target_net_state_dict[par] = (ETA*policy_net.state_dict()[par] +(1-ETA)*target_net.state_dict()[par])
    target_net.load_state_dict(target_net_state_dict)

    if (terminated or finished):
      if i_episode%10 == 0: print(f"Ep {i_episode}: Score {total_reward}")
      break

  reward_list.append(total_reward)
  avg_score = sum(reward_list[-WINDOW:])/WINDOW if len(reward_list) > WINDOW else sum(reward_list)/len(reward_list)
  if avg_score >= TARGET_SCORE:
    torch.save(policy_net.state_dict(), 'lunar_lander_dqn_with_transfer_weights.pth')
    break

import numpy as np
print("Saving datas...")
np.save('scores_WITH_transfer_no_adapter.npy', np.array(reward_list))
print("datas saved !.")
